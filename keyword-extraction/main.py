# main.py - í†µí•© í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤
import asyncio
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional, Set

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from aiokafka import AIOKafkaConsumer

from hybrid_keyword_extractor import HybridKeywordExtractor
from advanced_trend_analyzer import AdvancedTrendAnalyzer, TrendMetrics

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === ëª¨ë¸ ì •ì˜ ===
class KeywordRequest(BaseModel):
    title: str
    content: str
    category: Optional[str] = ""
    metadata: Optional[Dict] = {}

class TrendingResponse(BaseModel):
    keywords: List[Dict]
    alerts: List[Dict]
    total_processed: int

class WebSocketManager:
    def __init__(self):
        self.active_connections: Set[WebSocket] = set()
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)
        logger.info(f"WebSocket ì—°ê²°ë¨. ì´ {len(self.active_connections)}ê°œ ì—°ê²°")
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.discard(websocket)
        logger.info(f"WebSocket ì—°ê²° í•´ì œ. ì´ {len(self.active_connections)}ê°œ ì—°ê²°")
    
    async def broadcast(self, message: Dict):
        """ëª¨ë“  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ì— ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if not self.active_connections:
            return
        
        message_str = json.dumps(message, ensure_ascii=False)
        disconnected = set()
        
        for connection in self.active_connections:
            try:
                await connection.send_text(message_str)
            except Exception:
                disconnected.add(connection)
        
        # ëŠì–´ì§„ ì—°ê²° ì •ë¦¬
        self.active_connections -= disconnected

class KafkaArticleConsumer:
    def __init__(self, extractor: HybridKeywordExtractor, analyzer: AdvancedTrendAnalyzer, websocket_manager: WebSocketManager):
        self.extractor = extractor
        self.analyzer = analyzer
        self.websocket_manager = websocket_manager
        self.consumer = None
        self.running = False
        self.processed_count = 0
        
    async def start_consuming(self):
        """Kafka Consumer ì‹œì‘"""
        self.running = True
        
        try:
            self.consumer = AIOKafkaConsumer(
                'postgres.public.articles',
                bootstrap_servers='localhost:9092',
                group_id='keyword-extraction-service',
                auto_offset_reset='latest',
                value_deserializer=lambda m: json.loads(m.decode('utf-8'))
            )
            
            await self.consumer.start()
            logger.info("ğŸ“¡ Kafka Consumer ì‹œì‘ ì™„ë£Œ")
            
            async for message in self.consumer:
                if not self.running:
                    break
                await self._process_cdc_event(message.value)
                
        except Exception as e:
            logger.error(f"âŒ Kafka Consumer ì˜¤ë¥˜: {e}")
        finally:
            if self.consumer:
                await self.consumer.stop()
    
    async def _process_cdc_event(self, event_data: Dict):
        """CDC ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            if event_data.get('op') != 'c':
                return
                
            article_data = event_data.get('after', {})
            if not article_data or article_data.get('keywords'):
                return
                
            article_id = article_data.get('id')
            title = article_data.get('title', '')
            content = article_data.get('content', '')
            category = article_data.get('category', '')
            
            if not title or not content or len(content) < 50:
                return
                
            logger.info(f"ğŸ“„ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘: {article_id}")
            
            # í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
            metadata = {
                'views_count': article_data.get('views_count', 0),
                'category': category,
                'article_id': article_id
            }
            
            keywords = await self.extractor.extract_keywords(title, content, metadata)
            
            if keywords:
                # ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ì— ì¶”ê°€
                await self.analyzer.add_keywords(keywords, category, metadata)
                
                self.processed_count += 1
                
                # WebSocketìœ¼ë¡œ ì‹¤ì‹œê°„ ì „ì†¡
                await self._broadcast_new_keywords(article_id, title, category, keywords)
                
                logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
            
        except Exception as e:
            logger.error(f"CDC ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _broadcast_new_keywords(self, article_id: int, title: str, category: str, keywords: List[str]):
        """ìƒˆ í‚¤ì›Œë“œ WebSocket ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        message = {
            "type": "new_keywords",
            "data": {
                "article_id": article_id,
                "title": title,
                "category": category,
                "keywords": keywords,
                "timestamp": datetime.now().isoformat()
            }
        }
        await self.websocket_manager.broadcast(message)
    
    async def stop(self):
        self.running = False
        if self.consumer:
            await self.consumer.stop()

# === FastAPI ì•± ===
app = FastAPI(title="ê³ ë„í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
extractor = HybridKeywordExtractor()  # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
analyzer = AdvancedTrendAnalyzer()
websocket_manager = WebSocketManager()
kafka_consumer = KafkaArticleConsumer(extractor, analyzer, websocket_manager)

@app.on_event("startup")
async def startup():
    """ì„œë¹„ìŠ¤ ì‹œì‘ì‹œ ì´ˆê¸°í™”"""
    await extractor.initialize()
    await analyzer.initialize()
    
    # Kafka Consumerì™€ Redis ì•Œë¦¼ êµ¬ë… ì‹œì‘
    asyncio.create_task(kafka_consumer.start_consuming())
    asyncio.create_task(redis_alert_subscriber())
    
    logger.info("ğŸš€ ê³ ë„í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ")

@app.on_event("shutdown")
async def shutdown():
    await kafka_consumer.stop()

async def redis_alert_subscriber():
    """Redis ì•Œë¦¼ ì±„ë„ êµ¬ë… (WebSocket ë¸Œë¡œë“œìºìŠ¤íŠ¸ìš©)"""
    import redis.asyncio as redis
    
    try:
        redis_client = redis.from_url("redis://localhost:6379")
        pubsub = redis_client.pubsub()
        await pubsub.subscribe("alert_channel")
        
        async for message in pubsub.listen():
            if message["type"] == "message":
                alert_data = json.loads(message["data"])
                
                # WebSocketìœ¼ë¡œ ì•Œë¦¼ ë¸Œë¡œë“œìºìŠ¤íŠ¸
                await websocket_manager.broadcast({
                    "type": "alert",
                    "data": alert_data
                })
                
    except Exception as e:
        logger.error(f"Redis ì•Œë¦¼ êµ¬ë… ì˜¤ë¥˜: {e}")

@app.websocket("/ws/keywords")
async def websocket_endpoint(websocket: WebSocket):
    """ì‹¤ì‹œê°„ í‚¤ì›Œë“œ WebSocket"""
    await websocket_manager.connect(websocket)
    
    try:
        # ì—°ê²° ì‹œ í˜„ì¬ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì „ì†¡
        trending = await analyzer.get_trending_keywords_advanced(10)
        await websocket.send_text(json.dumps({
            "type": "initial_trending",
            "data": [
                {
                    "keyword": t.keyword,
                    "count": t.count_1h,
                    "trend": t.trend_direction,
                    "score": t.compound_score
                }
                for t in trending
            ]
        }, ensure_ascii=False))
        
        # ì—°ê²° ìœ ì§€
        while True:
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        websocket_manager.disconnect(websocket)

@app.get("/trending-keywords-advanced")
async def get_trending_keywords_advanced(limit: int = 20):
    """ê³ ë„í™”ëœ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¡°íšŒ"""
    trending = await analyzer.get_trending_keywords_advanced(limit)
    
    return {
        "keywords": [
            {
                "keyword": t.keyword,
                "counts": {
                    "1h": t.count_1h,
                    "6h": t.count_6h,
                    "24h": t.count_24h,
                    "7d": t.count_7d
                },
                "velocity": {
                    "1h": t.velocity_1h,
                    "6h": t.velocity_6h
                },
                "anomaly_score": t.anomaly_score,
                "z_score": t.z_score,
                "trend_direction": t.trend_direction,
                "compound_score": t.compound_score
            }
            for t in trending
        ],
        "total_count": len(trending)
    }

@app.get("/keyword-timeline/{keyword}")
async def get_keyword_timeline(keyword: str, hours: int = 24):
    """í‚¤ì›Œë“œ ì‹œê°„ë³„ ë°ì´í„° (ì°¨íŠ¸ìš©)"""
    timeline = await analyzer.get_timeline_data(keyword, hours)
    return {"keyword": keyword, "timeline": timeline}

@app.get("/alerts")
async def get_recent_alerts(limit: int = 10):
    """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
    alerts = await analyzer.get_recent_alerts(limit)
    return {"alerts": alerts}

@app.get("/stats")
async def get_service_stats():
    """ì„œë¹„ìŠ¤ í†µê³„"""
    extraction_stats = extractor.get_extraction_stats()
    
    return {
        "processed_articles": kafka_consumer.processed_count,
        "active_websockets": len(websocket_manager.active_connections),
        "extraction_stats": extraction_stats,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/extract-keywords")
async def manual_extract_keywords(request: KeywordRequest):
    """ìˆ˜ë™ í‚¤ì›Œë“œ ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ìš©)"""
    start_time = datetime.now()
    
    keywords = await extractor.extract_keywords(
        request.title, 
        request.content, 
        request.metadata
    )
    
    if keywords:
        await analyzer.add_keywords(keywords, request.category, request.metadata)
    
    processing_time = (datetime.now() - start_time).total_seconds()
    
    return {
        "keywords": keywords,
        "processing_time": processing_time,
        "extraction_method": "hybrid" if extractor.openai_client else "basic"
    }

@app.get("/health")
async def health_check():
    try:
        await analyzer.redis_client.ping()
        redis_status = True
    except:
        redis_status = False
    
    return {
        "status": "healthy" if redis_status else "degraded",
        "services": {
            "redis": redis_status,
            "kafka_consumer": kafka_consumer.running,
            "websocket_connections": len(websocket_manager.active_connections)
        },
        "processed_count": kafka_consumer.processed_count
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=True)