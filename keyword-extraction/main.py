# main.py - ì•ˆì •ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ (ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „)
import os
import json
import logging
import threading
import time
from typing import List, Dict, Optional, Set
from datetime import datetime
from dotenv import load_dotenv
import asyncio

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from confluent_kafka import Consumer, KafkaError, KafkaException

from hybrid_keyword_extractor import HybridKeywordExtractor
from advanced_trend_analyzer import AdvancedTrendAnalyzer, TrendMetrics
from realtime_keyword_aggregator import RealTimeKeywordAggregator, WordCloudData, WordCloudGenerator
from keyword_republisher import KafkaKeywordRepublisher, EnhancedKeywordProcessor


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
aggregator = RealTimeKeywordAggregator()
wordcloud_generator = WordCloudGenerator()

load_dotenv()
# í™˜ê²½ë³€ìˆ˜ì—ì„œ Kafka ì„¤ì • ì½ê¸°
KAFKA_BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS')
KAFKA_TOPIC = os.getenv('KAFKA_TOPIC')
KAFKA_GROUP_ID = os.getenv('KAFKA_GROUP_ID')

print(f"Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
print(f"Topic: {KAFKA_TOPIC}")
print(f"Group: {KAFKA_GROUP_ID}")

# === ëª¨ë¸ ì •ì˜ ===
class KeywordRequest(BaseModel):
    title: str
    content: str
    category: Optional[str] = ""
    metadata: Optional[Dict] = {}

class TrendingResponse(BaseModel):
    keywords: List[Dict]
    alerts: List[Dict]
    total_processed: int

class WebSocketManager:
    def __init__(self):
        self.active_connections: Set[WebSocket] = set()
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)
        logger.info(f"WebSocket ì—°ê²°ë¨. ì´ {len(self.active_connections)}ê°œ ì—°ê²°")
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.discard(websocket)
        logger.info(f"WebSocket ì—°ê²° í•´ì œ. ì´ {len(self.active_connections)}ê°œ ì—°ê²°")
    
    async def broadcast(self, message: Dict):
        """ëª¨ë“  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ì— ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if not self.active_connections:
            return
        
        message_str = json.dumps(message, ensure_ascii=False)
        disconnected = set()
        
        for connection in self.active_connections:
            try:
                await connection.send_text(message_str)
            except Exception:
                disconnected.add(connection)
        
        # ëŠì–´ì§„ ì—°ê²° ì •ë¦¬
        self.active_connections -= disconnected

class KafkaCDCEventHandler:
    """CDC ì´ë²¤íŠ¸ ì²˜ë¦¬ í´ë˜ìŠ¤ (ìˆ˜ì •ë¨)"""
    
    def __init__(self, extractor: HybridKeywordExtractor, analyzer: AdvancedTrendAnalyzer, websocket_manager: WebSocketManager):
        self.extractor = extractor
        self.analyzer = analyzer
        self.websocket_manager = websocket_manager
        self.processed_count = 0
        
        # ğŸ”¥ Kafka ì¬ë°œí–‰ê¸° ì´ˆê¸°í™” (ì˜µì…”ë„)
        self.keyword_processor = None
        
        try:
            bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS') or "localhost:9092"
            self.republisher = KafkaKeywordRepublisher(
                bootstrap_servers=bootstrap_servers,
                keyword_topic="processed-keywords"
            )
            self.keyword_processor = EnhancedKeywordProcessor(self.republisher)
            logger.info("âœ… í‚¤ì›Œë“œ ì¬ë°œí–‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì¬ë°œí–‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.republisher = None
            self.keyword_processor = None

    def process_event(self, event: Dict) -> bool:
        """CDC ì´ë²¤íŠ¸ ë™ê¸° ì²˜ë¦¬ (ìˆ˜ì •ë¨)"""
        try:
            # ì´ë²¤íŠ¸ êµ¬ì¡° í™•ì¸
            payload = event.get('payload', event)
            
            if not payload:
                return False
                
            # ì‘ì—… ìœ í˜• í™•ì¸ (create, updateë§Œ ì²˜ë¦¬)
            operation = payload.get('op')
            if operation not in ('c', 'r', 'u'):
                return True
                
            # ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ
            article_data = payload.get('after', {})
            if not article_data or not article_data.get('id'):
                return False
                
            title = article_data.get('title', '')
            content = article_data.get('content', '')
            category = article_data.get('category', '')
            article_id = article_data.get('id')
            
            if not title or not content or len(content) < 50:
                logger.debug(f"ê¸°ì‚¬ ë‚´ìš© ë¶€ì¡±: {article_id}")
                return True
                
            logger.info(f"ğŸ”¥ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘: {article_id} - {title[:50]}")

            # ğŸ”§ ë¹„ë™ê¸° í‚¤ì›Œë“œ ì¶”ì¶œì„ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            try:
                # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    keywords = loop.run_until_complete(
                        self._extract_keywords_async(title, content, category, article_id)
                    )
                    
                    if keywords:
                        self.processed_count += 1
                        
                        # ë¹„ë™ê¸° ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                        threading.Thread(
                            target=self._handle_async_tasks,
                            args=(article_id, title, content, category, keywords)
                        ).start()
                        
                        logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(keywords)}ê°œ - {keywords[:5]}")
                    else:
                        logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {article_id}")
                        
                finally:
                    loop.close()
                    
            except Exception as e:
                logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return False
    
    async def _extract_keywords_async(self, title: str, content: str, category: str, article_id: int):
        """í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        metadata = {
            'category': category,
            'article_id': article_id
        }
        
        return await self.extractor.extract_keywords(title, content, metadata)
    
    def _handle_async_tasks(self, article_id: int, title: str, content: str, category: str, keywords: List[str]):
        """ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ (ë³„ë„ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ)"""
        loop = None
        try:
            # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰
            loop.run_until_complete(
                self._async_tasks_with_republish(article_id, title, content, category, keywords)
            )
                
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        finally:
            # ì´ë²¤íŠ¸ ë£¨í”„ ì•ˆì „í•˜ê²Œ ì •ë¦¬
            if loop and not loop.is_closed():
                try:
                    pending = asyncio.all_tasks(loop)
                    for task in pending:
                        task.cancel()
                    
                    loop.run_until_complete(loop.shutdown_asyncgens())
                    loop.close()
                except Exception as cleanup_error:
                    logger.error(f"ì´ë²¤íŠ¸ ë£¨í”„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {cleanup_error}")
    
    async def _async_tasks_with_republish(self, article_id: int, title: str, content: str, 
                                        category: str, keywords: List[str]):
        """ì¬ë°œí–‰ ê¸°ëŠ¥ì´ í¬í•¨ëœ ë¹„ë™ê¸° ì‘ì—…ë“¤"""
        try:
            # 1. ê¸°ì¡´ íŠ¸ë Œë“œ ë¶„ì„
            metadata = {'article_id': article_id}
            await self.analyzer.add_keywords(keywords, category, metadata)
            
            # 2. ì‹¤ì‹œê°„ ì›Œë“œí´ë¼ìš°ë“œ ì§‘ê³„
            await aggregator.on_cdc_event(keywords, category)
            
            # 3. ğŸ”¥ í‚¤ì›Œë“œ ì²˜ë¦¬ ë° Kafka ì¬ë°œí–‰ (ì˜µì…”ë„)
            republish_success = False
            if self.keyword_processor:
                try:
                    confidence_scores = getattr(self.extractor, 'last_confidence_scores', {})
                    republish_success = await self.keyword_processor.process_and_republish(
                        article_id=article_id,
                        title=title,
                        content=content,
                        category=category,
                        raw_keywords=keywords,
                        confidence_scores=confidence_scores
                    )
                    
                    if republish_success:
                        logger.info(f"ğŸš€ í‚¤ì›Œë“œ ì¬ë°œí–‰ ì„±ê³µ: article_{article_id}")
                    else:
                        logger.error(f"âŒ í‚¤ì›Œë“œ ì¬ë°œí–‰ ì‹¤íŒ¨: article_{article_id}")
                except Exception as e:
                    logger.error(f"ì¬ë°œí–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 4. WebSocket ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ì¬ë°œí–‰ ìƒíƒœ í¬í•¨)
            message = {
                "type": "new_keywords",
                "data": {
                    "article_id": article_id,
                    "title": title,
                    "category": category,
                    "keywords": keywords,
                    "republished": republish_success,
                    "timestamp": datetime.now().isoformat()
                }
            }
            await self.websocket_manager.broadcast(message)
            
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")

class StableKafkaConsumer:
    """ë””ë²„ê¹…ì´ ê°•í™”ëœ Kafka Consumer"""
    
    def __init__(self, event_handler: KafkaCDCEventHandler):
        self.event_handler = event_handler
        self.running = False
        self.consumer = None
        self.worker_thread = None
        
        # Kafka ì„¤ì •
        self.kafka_config = {
            'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
            'group.id': KAFKA_GROUP_ID,
            'auto.offset.reset': 'latest',
            'enable.auto.commit': True,
            'auto.commit.interval.ms': 1000,
            'max.poll.interval.ms': 300000,
            'session.timeout.ms': 30000,
            'heartbeat.interval.ms': 3000,
        }
        
        self.topics = [KAFKA_TOPIC]
        
    def start(self):
        """Consumer ì‹œì‘"""
        if self.running:
            logger.warning("ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ Consumerê°€ ìˆìŠµë‹ˆë‹¤")
            return
            
        self.running = True
        self.worker_thread = threading.Thread(target=self._consume_loop)
        self.worker_thread.daemon = True
        self.worker_thread.start()
        logger.info(f"ğŸš€ Kafka Consumer ì‹œì‘: {KAFKA_BOOTSTRAP_SERVERS}")
        
    def stop(self):
        """Consumer ì¤‘ì§€"""
        self.running = False
        if self.worker_thread:
            logger.info("Kafka Consumer ì¤‘ì§€ ì¤‘...")
            self.worker_thread.join(timeout=30)
            logger.info("Kafka Consumer ì¤‘ì§€ ì™„ë£Œ")
            
    def _consume_loop(self):
        """ë©”ì‹œì§€ ì†Œë¹„ ë£¨í”„"""
        try:
            logger.info(f"ğŸ”§ Kafka ì„¤ì •: {self.kafka_config}")
            
            self.consumer = Consumer(self.kafka_config)
            self.consumer.subscribe(self.topics)
            logger.info(f"ğŸ“¡ í† í”½ êµ¬ë… ì™„ë£Œ: {self.topics}")
            
            # Consumer ë©”íƒ€ë°ì´í„° í™•ì¸
            metadata = self.consumer.list_topics(timeout=10)
            logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ í† í”½: {list(metadata.topics.keys())}")
            
            if KAFKA_TOPIC in metadata.topics:
                topic_metadata = metadata.topics[KAFKA_TOPIC]
                logger.info(f"ğŸ“ í† í”½ '{KAFKA_TOPIC}' íŒŒí‹°ì…˜ ìˆ˜: {len(topic_metadata.partitions)}")
            else:
                logger.error(f"âŒ í† í”½ '{KAFKA_TOPIC}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                return
            
            message_count = 0
            poll_count = 0
            
            while self.running:
                try:
                    poll_count += 1
                    msg = self.consumer.poll(timeout=5.0)
                    
                    if poll_count % 10 == 0:
                        logger.info(f"â° í´ë§ #{poll_count} - ë©”ì‹œì§€: {'ìˆìŒ' if msg else 'ì—†ìŒ'}")
                    
                    if msg is None:
                        continue
                        
                    if msg.error():
                        if msg.error().code() == KafkaError._PARTITION_EOF:
                            logger.debug(f"íŒŒí‹°ì…˜ ë ë„ë‹¬: {msg.topic()}-{msg.partition()}")
                        else:
                            logger.error(f"Kafka ì˜¤ë¥˜: {msg.error()}")
                        continue
                    
                    message_count += 1
                    logger.info(f"ğŸ”¥ğŸ“¨ ë©”ì‹œì§€ #{message_count} ìˆ˜ì‹ !")
                    
                    try:
                        value = msg.value()
                        if value:
                            event = json.loads(value.decode('utf-8'))
                            
                            # ì´ë²¤íŠ¸ ì²˜ë¦¬
                            success = self.event_handler.process_event(event)
                            logger.info(f"{'âœ…' if success else 'âŒ'} ì´ë²¤íŠ¸ ì²˜ë¦¬ {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
                                
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                    except Exception as e:
                        logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
                        continue
                        
                except KafkaException as e:
                    logger.error(f"Kafka ì˜ˆì™¸: {e}")
                    time.sleep(5)
                except Exception as e:
                    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
                    time.sleep(1)
                    
        except Exception as e:
            logger.error(f"Consumer ë£¨í”„ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        finally:
            if self.consumer:
                try:
                    self.consumer.close()
                except Exception as e:
                    logger.error(f"Consumer ì¢…ë£Œ ì˜¤ë¥˜: {e}")

# === FastAPI ì•± ===
app = FastAPI(title="ì•ˆì •ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤", version="3.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
extractor = HybridKeywordExtractor()
analyzer = AdvancedTrendAnalyzer()
websocket_manager = WebSocketManager()
event_handler = KafkaCDCEventHandler(extractor, analyzer, websocket_manager)
kafka_consumer = StableKafkaConsumer(event_handler)

@app.on_event("startup")
async def startup():
    """ì„œë¹„ìŠ¤ ì‹œì‘ì‹œ ì´ˆê¸°í™”"""
    logger.info("ğŸš€ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    await extractor.initialize()
    await analyzer.initialize(redis_url="redis://:homesweethome@localhost:6379")
    await aggregator.initialize(redis_url="redis://:homesweethome@localhost:6379")
    
    # ì›Œë“œí´ë¼ìš°ë“œ ì—…ë°ì´íŠ¸ ì½œë°± ë“±ë¡
    async def broadcast_wordcloud_update(wordcloud_data):
        serializable_data = {}
        for window_type, data in wordcloud_data.items():
            if isinstance(data, WordCloudData):
                serializable_data[window_type] = {
                    "keywords": data.keywords,
                    "window_type": data.window_type,
                    "timestamp": data.timestamp.isoformat(),
                    "total_count": data.total_count,
                    "unique_keywords": data.unique_keywords,
                    "top_keywords": data.top_keywords
                }
            else:
                serializable_data[window_type] = data
                
        await websocket_manager.broadcast({
            "type": "wordcloud_update",
            "data": serializable_data
        })
    
    aggregator.add_update_callback(broadcast_wordcloud_update)
    
    # Kafka Consumer ì‹œì‘
    kafka_consumer.start()
    
    # ì£¼ê¸°ì  ì›Œë“œí´ë¼ìš°ë“œ ì—…ë°ì´íŠ¸ íƒœìŠ¤í¬ ì¶”ê°€
    asyncio.create_task(periodic_wordcloud_update())
    
    logger.info("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ")

async def periodic_wordcloud_update():
    """ì£¼ê¸°ì  ì›Œë“œí´ë¼ìš°ë“œ ì—…ë°ì´íŠ¸"""
    logger.info("ğŸ”„ ì£¼ê¸°ì  ì›Œë“œí´ë¼ìš°ë“œ ì—…ë°ì´íŠ¸ ì‹œì‘")
    
    while True:
        try:
            await asyncio.sleep(60)  # 1ë¶„ ëŒ€ê¸°
            
            all_wordcloud_data = {}
            
            for window_type in ["30min", "1h", "6h"]:
                try:
                    wordcloud_data = await aggregator.get_current_wordcloud(window_type)
                    layout = wordcloud_generator.generate_wordcloud_layout(wordcloud_data)
                    
                    all_wordcloud_data[window_type] = {
                        "words": layout.get("words", []),
                        "window_type": window_type,
                        "timestamp": datetime.now().isoformat(),
                        "total_count": wordcloud_data.total_count,
                        "unique_keywords": wordcloud_data.unique_keywords
                    }
                    
                except Exception as e:
                    logger.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜ ({window_type}): {e}")
                    continue
            
            if all_wordcloud_data:
                await websocket_manager.broadcast({
                    "type": "wordcloud_update",
                    "data": all_wordcloud_data
                })
                
                logger.info(f"ğŸ“¡ ì£¼ê¸°ì  ì›Œë“œí´ë¼ìš°ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            await asyncio.sleep(10)

@app.on_event("shutdown")
async def shutdown():
    """ì„œë¹„ìŠ¤ ì¢…ë£Œ"""
    logger.info("ğŸ›‘ ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")
    kafka_consumer.stop()
    
    # ì¬ë°œí–‰ê¸° ì¢…ë£Œ
    if event_handler.republisher:
        event_handler.republisher.close()
    
    logger.info("âœ… ì„œë¹„ìŠ¤ ì¢…ë£Œ ì™„ë£Œ")

@app.websocket("/ws/keywords")
async def websocket_endpoint(websocket: WebSocket):
    """ì‹¤ì‹œê°„ í‚¤ì›Œë“œ WebSocket"""
    await websocket_manager.connect(websocket)
    
    stats_task = None
    try:
        # ì—°ê²° ì‹œ í˜„ì¬ ë°ì´í„° ì „ì†¡
        for window_type in ["30min", "1h", "6h"]:
            try:
                wordcloud_data = await aggregator.get_current_wordcloud(window_type)
                layout = wordcloud_generator.generate_wordcloud_layout(wordcloud_data)
                
                await websocket.send_text(json.dumps({
                    "type": "wordcloud_update",
                    "data": {window_type: layout}
                }, ensure_ascii=False))
            except Exception as e:
                logger.error(f"ì´ˆê¸° ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ì „ì†¡ ì˜¤ë¥˜: {e}")
        
        # í†µê³„ ì—…ë°ì´íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
        stats_task = asyncio.create_task(stream_stats(websocket))
        
        # ì—°ê²° ìœ ì§€
        while True:
            try:
                message = await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
                logger.debug(f"WebSocket ë©”ì‹œì§€ ìˆ˜ì‹ : {message}")
            except asyncio.TimeoutError:
                try:
                    await websocket.send_text(json.dumps({
                        "type": "ping",
                        "timestamp": datetime.now().isoformat()
                    }))
                except Exception:
                    break
                        
    except WebSocketDisconnect:
        logger.info("í´ë¼ì´ì–¸íŠ¸ê°€ WebSocket ì—°ê²°ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"WebSocket ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
    finally:
        if stats_task and not stats_task.done():
            stats_task.cancel()
        websocket_manager.disconnect(websocket)

async def stream_stats(websocket: WebSocket):
    """ì£¼ê¸°ì  í†µê³„ ì—…ë°ì´íŠ¸"""
    try:
        while True:
            stats = {
                "processed_articles": event_handler.processed_count,
                "active_keywords": len((await aggregator.get_current_wordcloud("30min")).keywords),
                "timestamp": datetime.now().isoformat()
            }
            
            await websocket.send_text(json.dumps({
                "type": "stats_update",
                "data": stats
            }))
            
            await asyncio.sleep(5)
            
    except Exception as e:
        logger.error(f"í†µê³„ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")

# === API ì—”ë“œí¬ì¸íŠ¸ë“¤ ===
@app.get("/trending-keywords-advanced")
async def get_trending_keywords_advanced(limit: int = 20):
    """ê³ ë„í™”ëœ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¡°íšŒ"""
    trending = await analyzer.get_trending_keywords_advanced(limit)
    
    return {
        "keywords": [
            {
                "keyword": t.keyword,
                "counts": {
                    "1h": t.count_1h,
                    "6h": t.count_6h,
                    "24h": t.count_24h,
                    "7d": t.count_7d
                },
                "velocity": {
                    "1h": t.velocity_1h,
                    "6h": t.velocity_6h
                },
                "anomaly_score": t.anomaly_score,
                "z_score": t.z_score,
                "trend_direction": t.trend_direction,
                "compound_score": t.compound_score
            }
            for t in trending
        ],
        "total_count": len(trending)
    }

@app.get("/keyword-timeline/{keyword}")
async def get_keyword_timeline(keyword: str, hours: int = 24):
    """í‚¤ì›Œë“œ ì‹œê°„ë³„ ë°ì´í„°"""
    timeline = await analyzer.get_timeline_data(keyword, hours)
    return {"keyword": keyword, "timeline": timeline}

@app.get("/alerts")
async def get_recent_alerts(limit: int = 10):
    """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
    alerts = await analyzer.get_recent_alerts(limit)
    return {"alerts": alerts}

@app.post("/extract-keywords")
async def manual_extract_keywords(request: KeywordRequest):
    """ìˆ˜ë™ í‚¤ì›Œë“œ ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ìš©)"""
    start_time = datetime.now()
    
    keywords = await extractor.extract_keywords(
        request.title, 
        request.content, 
        request.metadata
    )
    
    if keywords:
        await analyzer.add_keywords(keywords, request.category, request.metadata)
    
    processing_time = (datetime.now() - start_time).total_seconds()
    
    return {
        "keywords": keywords,
        "processing_time": processing_time,
        "extraction_method": "hybrid" if extractor.openai_client else "basic"
    }

@app.get("/wordcloud/{window_type}")
async def get_wordcloud(window_type: str = "30min"):
    """ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ì¡°íšŒ"""
    if window_type not in ["30min", "1h", "6h"]:
        return {"error": "Invalid window type"}
    
    wordcloud_data = await aggregator.get_current_wordcloud(window_type)
    layout = wordcloud_generator.generate_wordcloud_layout(wordcloud_data)
    
    return layout

@app.get("/wordcloud/compare")
async def compare_windows():
    """ë‹¤ì¤‘ ìœˆë„ìš° ë¹„êµ"""
    comparison = {}
    
    for window_type in ["30min", "1h", "6h"]:
        wordcloud_data = await aggregator.get_current_wordcloud(window_type)
        comparison[window_type] = {
            "total_count": wordcloud_data.total_count,
            "unique_keywords": wordcloud_data.unique_keywords,
            "top_5": wordcloud_data.top_keywords[:5]
        }
    
    return comparison

# ğŸ”¥ ì¬ë°œí–‰ê¸° ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ (ì˜µì…”ë„)
@app.get("/republisher/stats")
async def get_republisher_stats():
    """í‚¤ì›Œë“œ ì¬ë°œí–‰ í†µê³„"""
    if event_handler.republisher:
        return {
            "republisher_stats": event_handler.republisher.get_stats(),
            "processed_articles": event_handler.processed_count,
            "available": True,
            "timestamp": datetime.now().isoformat()
        }
    else:
        return {
            "available": False,
            "message": "ì¬ë°œí–‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().isoformat()
        }

@app.get("/stats")
async def get_service_stats():
    """ì„œë¹„ìŠ¤ í†µê³„ (ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸)"""
    extraction_stats = extractor.get_extraction_stats()
    
    return {
        "processed_articles": event_handler.processed_count,
        "active_websockets": len(websocket_manager.active_connections),
        "extraction_stats": extraction_stats,
        "kafka_running": kafka_consumer.running,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        await analyzer.redis_client.ping()
        redis_status = True
    except:
        redis_status = False
    
    return {
        "status": "healthy" if (redis_status and kafka_consumer.running) else "degraded",
        "services": {
            "redis": redis_status,
            "kafka_consumer": kafka_consumer.running,
            "websocket_connections": len(websocket_manager.active_connections),
        },
        "kafka_config": {
            "bootstrap_servers": KAFKA_BOOTSTRAP_SERVERS,
            "topic": KAFKA_TOPIC,
            "group_id": KAFKA_GROUP_ID
        },
        "processed_count": event_handler.processed_count
    }

@app.get("/debug/kafka-status")
async def debug_kafka_status():
    """Kafka ìƒíƒœ ë””ë²„ê¹…"""
    return {
        "consumer_running": kafka_consumer.running,
        "worker_thread_alive": kafka_consumer.worker_thread.is_alive() if kafka_consumer.worker_thread else False,
        "processed_messages": event_handler.processed_count,
        "config": kafka_consumer.kafka_config,
        "topics": kafka_consumer.topics,
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=False)