# main.py - ì•ˆì •ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ (confluent_kafka ì‚¬ìš©)
import os
import json
import logging
import threading
import time
from typing import List, Dict, Optional, Set
from datetime import datetime
from dotenv import load_dotenv
import asyncio

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from confluent_kafka import Consumer, KafkaError, KafkaException

from hybrid_keyword_extractor import HybridKeywordExtractor
from advanced_trend_analyzer import AdvancedTrendAnalyzer, TrendMetrics
from realtime_keyword_aggregator import RealTimeKeywordAggregator, WordCloudData, WordCloudGenerator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
aggregator = RealTimeKeywordAggregator()
wordcloud_generator = WordCloudGenerator()


load_dotenv()
# í™˜ê²½ë³€ìˆ˜ì—ì„œ Kafka ì„¤ì • ì½ê¸°
KAFKA_BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS')
KAFKA_TOPIC = os.getenv('KAFKA_TOPIC')
KAFKA_GROUP_ID = os.getenv('KAFKA_GROUP_ID')

print(f"Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
print(f"Topic: {KAFKA_TOPIC}")
print(f"Group: {KAFKA_GROUP_ID}")
# === ëª¨ë¸ ì •ì˜ ===
class KeywordRequest(BaseModel):
    title: str
    content: str
    category: Optional[str] = ""
    metadata: Optional[Dict] = {}

class TrendingResponse(BaseModel):
    keywords: List[Dict]
    alerts: List[Dict]
    total_processed: int

class WebSocketManager:
    def __init__(self):
        self.active_connections: Set[WebSocket] = set()
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)
        logger.info(f"WebSocket ì—°ê²°ë¨. ì´ {len(self.active_connections)}ê°œ ì—°ê²°")
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.discard(websocket)
        logger.info(f"WebSocket ì—°ê²° í•´ì œ. ì´ {len(self.active_connections)}ê°œ ì—°ê²°")
    
    async def broadcast(self, message: Dict):
        """ëª¨ë“  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ì— ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if not self.active_connections:
            return
        
        message_str = json.dumps(message, ensure_ascii=False)
        disconnected = set()
        
        for connection in self.active_connections:
            try:
                await connection.send_text(message_str)
            except Exception:
                disconnected.add(connection)
        
        # ëŠì–´ì§„ ì—°ê²° ì •ë¦¬
        self.active_connections -= disconnected

class KafkaCDCEventHandler:
    """CDC ì´ë²¤íŠ¸ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, extractor: HybridKeywordExtractor, analyzer: AdvancedTrendAnalyzer, websocket_manager: WebSocketManager):
        self.extractor = extractor
        self.analyzer = analyzer
        self.websocket_manager = websocket_manager
        self.processed_count = 0
        
    def process_event(self, event: Dict) -> bool:
        """CDC ì´ë²¤íŠ¸ ë™ê¸° ì²˜ë¦¬"""
        try:
            # ì´ë²¤íŠ¸ êµ¬ì¡° í™•ì¸
            payload = event.get('payload', event)
            if not payload:
                return False
                
            # ì‘ì—… ìœ í˜• í™•ì¸ (create, updateë§Œ ì²˜ë¦¬)
            operation = payload.get('op')
            if operation not in ('c', 'r', 'u'):
                return True
                
            # ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ
            article_data = payload.get('after', {})
            if not article_data or not article_data.get('id'):
                return False
                
            title = article_data.get('title', '')
            content = article_data.get('content', '')
            category = article_data.get('category', '')
            article_id = article_data.get('id')
            
            if not title or not content or len(content) < 50:
                logger.debug(f"ê¸°ì‚¬ ë‚´ìš© ë¶€ì¡±: {article_id}")
                return True
                
            logger.info(f"ğŸ”¥ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘: {article_id} - {title[:50]}")
                        # ë§ˆì§€ë§‰ ë¶€ë¶„
            logger.info(f"âœ… ì´ë²¤íŠ¸ ì²˜ë¦¬ ì„±ê³µ")
            

            # ğŸ”§ ë™ê¸° ë°©ì‹ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (asyncio.run ì‚¬ìš©)
            try:
                keywords = asyncio.run(self._extract_keywords_sync(title, content, category, article_id))
                
                if keywords:
                    self.processed_count += 1
                    
                    # ë¹„ë™ê¸° ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                    threading.Thread(
                        target=self._handle_async_tasks,
                        args=(article_id, title, category, keywords)
                    ).start()
                    
                    logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(keywords)}ê°œ - {keywords[:5]}")
                else:
                    logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {article_id}")
                    
            except Exception as e:
                logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return False
    
    async def _extract_keywords_sync(self, title: str, content: str, category: str, article_id: int):
        """í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        metadata = {
            'category': category,
            'article_id': article_id
        }
        
        return await self.extractor.extract_keywords(title, content, metadata)
    
    def _handle_async_tasks(self, article_id: int, title: str, category: str, keywords: List[str]):
        """ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ (ë³„ë„ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ)"""
        try:
            # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰
                loop.run_until_complete(self._async_tasks(article_id, title, category, keywords))
            finally:
                # ì´ë²¤íŠ¸ ë£¨í”„ ì •ë¦¬
                loop.close()
                
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    async def _async_tasks(self, article_id: int, title: str, category: str, keywords: List[str]):
        """ì‹¤ì œ ë¹„ë™ê¸° ì‘ì—…ë“¤"""
        try:
            # íŠ¸ë Œë“œ ë¶„ì„ì— í‚¤ì›Œë“œ ì¶”ê°€
            metadata = {'article_id': article_id}
            await self.analyzer.add_keywords(keywords, category, metadata)
            
            # ğŸ”¥ ì‹¤ì‹œê°„ ì›Œë“œí´ë¼ìš°ë“œ ì§‘ê³„ ì¶”ê°€
            await aggregator.on_cdc_event(keywords, category)
            
            # WebSocket ë¸Œë¡œë“œìºìŠ¤íŠ¸
            message = {
                "type": "new_keywords",
                "data": {
                    "article_id": article_id,
                    "title": title,
                    "category": category,
                    "keywords": keywords,
                    "timestamp": datetime.now().isoformat()
                }
            }
            await self.websocket_manager.broadcast(message)
            
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")

class StableKafkaConsumer:
    """ë””ë²„ê¹…ì´ ê°•í™”ëœ Kafka Consumer"""
    
    def __init__(self, event_handler: KafkaCDCEventHandler):
        self.event_handler = event_handler
        self.running = False
        self.consumer = None
        self.worker_thread = None
        
        # ğŸ”¥ ë””ë²„ê¹…ì„ ìœ„í•œ ì„¤ì • ë³€ê²½
        self.kafka_config = {
            'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
            'group.id': KAFKA_GROUP_ID,
            'auto.offset.reset': 'latest',  # ì²˜ìŒë¶€í„° ì½ê¸°
            'enable.auto.commit': True,
            'auto.commit.interval.ms': 1000,
            'max.poll.interval.ms': 300000,
            'session.timeout.ms': 30000,
            'heartbeat.interval.ms': 3000,
            # 'debug': 'consumer,cgrp,topic,fetch',  # ğŸ”¥ ë””ë²„ê¹… í™œì„±í™”
        }
        
        self.topics = [KAFKA_TOPIC]
        
    def start(self):
        """Consumer ì‹œì‘"""
        if self.running:
            logger.warning("ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ Consumerê°€ ìˆìŠµë‹ˆë‹¤")
            return
            
        self.running = True
        self.worker_thread = threading.Thread(target=self._consume_loop)
        self.worker_thread.daemon = True
        self.worker_thread.start()
        logger.info(f"ğŸš€ Kafka Consumer ì‹œì‘: {KAFKA_BOOTSTRAP_SERVERS}")
        
    def stop(self):
        """Consumer ì¤‘ì§€"""
        self.running = False
        if self.worker_thread:
            logger.info("Kafka Consumer ì¤‘ì§€ ì¤‘...")
            self.worker_thread.join(timeout=30)
            logger.info("Kafka Consumer ì¤‘ì§€ ì™„ë£Œ")
            
    def _consume_loop(self):
        """ë©”ì‹œì§€ ì†Œë¹„ ë£¨í”„ (ë””ë²„ê¹… ê°•í™”)"""
        try:
            logger.info(f"ğŸ”§ Kafka ì„¤ì •: {self.kafka_config}")
            
            self.consumer = Consumer(self.kafka_config)
            self.consumer.subscribe(self.topics)
            logger.info(f"ğŸ“¡ í† í”½ êµ¬ë… ì™„ë£Œ: {self.topics}")
            
            # ğŸ”¥ Consumer ë©”íƒ€ë°ì´í„° í™•ì¸
            metadata = self.consumer.list_topics(timeout=10)
            logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ í† í”½: {list(metadata.topics.keys())}")
            
            if KAFKA_TOPIC in metadata.topics:
                topic_metadata = metadata.topics[KAFKA_TOPIC]
                logger.info(f"ğŸ“ í† í”½ '{KAFKA_TOPIC}' íŒŒí‹°ì…˜ ìˆ˜: {len(topic_metadata.partitions)}")
            else:
                logger.error(f"âŒ í† í”½ '{KAFKA_TOPIC}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                return
            
            message_count = 0
            poll_count = 0
            
            while self.running:
                try:
                    poll_count += 1
                    
                    # ğŸ”¥ ë©”ì‹œì§€ í´ë§ (ë” ê¸´ íƒ€ì„ì•„ì›ƒ)
                    msg = self.consumer.poll(timeout=5.0)
                    
                    # ğŸ”¥ í´ë§ ìƒíƒœ ë¡œê¹… (ë§¤ 10ë²ˆë§ˆë‹¤)
                    if poll_count % 10 == 0:
                        logger.info(f"â° í´ë§ #{poll_count} - ë©”ì‹œì§€: {'ìˆìŒ' if msg else 'ì—†ìŒ'}")
                        
                        # Consumer í• ë‹¹ ìƒíƒœ í™•ì¸
                        assignment = self.consumer.assignment()
                        for partition in assignment:
                            logger.info(f"ğŸ“ íŒŒí‹°ì…˜: {partition.topic}[{partition.partition}] offset={partition.offset}")
                        
                        # ê° íŒŒí‹°ì…˜ì˜ ì˜¤í”„ì…‹ í™•ì¸
                        for partition in assignment:
                            try:
                                position = self.consumer.position([partition])
                                committed = self.consumer.committed([partition])
                                logger.info(f"ğŸ“Š íŒŒí‹°ì…˜ {partition}: position={position}, committed={committed}")
                            except Exception as e:
                                logger.error(f"ì˜¤í”„ì…‹ í™•ì¸ ì‹¤íŒ¨: {e}")
                    
                    if msg is None:
                        continue
                        
                    if msg.error():
                        if msg.error().code() == KafkaError._PARTITION_EOF:
                            logger.debug(f"íŒŒí‹°ì…˜ ë ë„ë‹¬: {msg.topic()}-{msg.partition()}")
                        else:
                            logger.error(f"Kafka ì˜¤ë¥˜: {msg.error()}")
                        continue
                    
                    # ğŸ”¥ ë©”ì‹œì§€ ìˆ˜ì‹  ë¡œê¹… ê°•í™”
                    message_count += 1
                    logger.info(f"ğŸ”¥ğŸ“¨ ë©”ì‹œì§€ #{message_count} ìˆ˜ì‹ !")
                    logger.info(f"  â”œâ”€ í† í”½: {msg.topic()}")
                    logger.info(f"  â”œâ”€ íŒŒí‹°ì…˜: {msg.partition()}")
                    logger.info(f"  â”œâ”€ ì˜¤í”„ì…‹: {msg.offset()}")
                    logger.info(f"  â””â”€ í¬ê¸°: {len(msg.value())} bytes")
                    
                    # ğŸ”¥ ë©”ì‹œì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                    try:
                        value = msg.value()
                        if value:
                            preview = value.decode('utf-8')[:200]
                            logger.info(f"ğŸ“„ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {preview}...")
                            
                            event = json.loads(value.decode('utf-8'))
                            
                            # ğŸ”¥ ì´ë²¤íŠ¸ êµ¬ì¡° ë¡œê¹…
                            payload = event.get('payload', {})
                            op = payload.get('op', 'unknown')
                            after = payload.get('after', {})
                            article_id = after.get('id', 'unknown')
                            title = after.get('title', 'unknown')[:30]
                            
                            logger.info(f"ğŸ” ì´ë²¤íŠ¸ ë¶„ì„:")
                            logger.info(f"  â”œâ”€ ì‘ì—…: {op}")
                            logger.info(f"  â”œâ”€ ê¸°ì‚¬ ID: {article_id}")
                            logger.info(f"  â””â”€ ì œëª©: {title}...")
                            
                            # ì´ë²¤íŠ¸ ì²˜ë¦¬
                            success = self.event_handler.process_event(event)
                            logger.info(f"{'âœ…' if success else 'âŒ'} ì´ë²¤íŠ¸ ì²˜ë¦¬ {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
                                
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                    except Exception as e:
                        logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
                        continue
                        
                except KafkaException as e:
                    logger.error(f"Kafka ì˜ˆì™¸: {e}")
                    time.sleep(5)
                except Exception as e:
                    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
                    time.sleep(1)
                    
        except Exception as e:
            logger.error(f"Consumer ë£¨í”„ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        finally:
            if self.consumer:
                try:
                    self.consumer.close()
                except Exception as e:
                    logger.error(f"Consumer ì¢…ë£Œ ì˜¤ë¥˜: {e}")


# === FastAPI ì•± ===
app = FastAPI(title="ì•ˆì •ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤", version="3.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
extractor = HybridKeywordExtractor()
analyzer = AdvancedTrendAnalyzer()
websocket_manager = WebSocketManager()
event_handler = KafkaCDCEventHandler(extractor, analyzer, websocket_manager)
kafka_consumer = StableKafkaConsumer(event_handler)
# ğŸ”¥ ì¶”ê°€: Consumer ê·¸ë£¹ ë¦¬ì…‹ í•¨ìˆ˜
def reset_consumer_group():
    """Consumer ê·¸ë£¹ ì˜¤í”„ì…‹ ë¦¬ì…‹"""
    try:
        from confluent_kafka.admin import AdminClient, ConfigResource
        
        admin_client = AdminClient({
            'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS
        })
        
        # Consumer ê·¸ë£¹ ì •ë³´ í™•ì¸
        logger.info(f"ğŸ”§ Consumer ê·¸ë£¹ '{KAFKA_GROUP_ID}' ë¦¬ì…‹ ì‹œë„")
        
        # ìƒˆë¡œìš´ Consumerë¡œ ì˜¤í”„ì…‹ ë¦¬ì…‹
        reset_consumer = Consumer({
            'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
            'group.id': KAFKA_GROUP_ID + '-reset',
            'auto.offset.reset': 'earliest'
        })
        
        reset_consumer.subscribe([KAFKA_TOPIC])
        
        # íŒŒí‹°ì…˜ í• ë‹¹ ëŒ€ê¸°
        msg = reset_consumer.poll(timeout=10.0)
        assignment = reset_consumer.assignment()
        
        if assignment:
            logger.info(f"ğŸ“ ë¦¬ì…‹ìš© íŒŒí‹°ì…˜ í• ë‹¹: {assignment}")
            
            # ê° íŒŒí‹°ì…˜ì˜ ì‹œì‘ ì˜¤í”„ì…‹ìœ¼ë¡œ ì´ë™
            for partition in assignment:
                low, high = reset_consumer.get_watermark_offsets(partition, timeout=10.0)
                logger.info(f"ğŸ“Š íŒŒí‹°ì…˜ {partition}: low={low}, high={high}")
        
        reset_consumer.close()
        logger.info("âœ… Consumer ê·¸ë£¹ ë¦¬ì…‹ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ Consumer ê·¸ë£¹ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
# startup ì´ë²¤íŠ¸ì— ì¶”ê°€
@app.on_event("startup")
async def startup():
    """ì„œë¹„ìŠ¤ ì‹œì‘ì‹œ ì´ˆê¸°í™”"""
    logger.info("ğŸš€ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    await extractor.initialize()
    await analyzer.initialize(redis_url="redis://:homesweethome@localhost:6379")
    await aggregator.initialize(redis_url="redis://:homesweethome@localhost:6379")
    
    # ì›Œë“œí´ë¼ìš°ë“œ ì—…ë°ì´íŠ¸ ì½œë°± ë“±ë¡ - Dictë¡œ ë³€í™˜
    async def broadcast_wordcloud_update(wordcloud_data):
        serializable_data = {}
        for window_type, data in wordcloud_data.items():
            if isinstance(data, WordCloudData):
                serializable_data[window_type] = {
                    "keywords": data.keywords,
                    "window_type": data.window_type,
                    "timestamp": data.timestamp.isoformat(),
                    "total_count": data.total_count,
                    "unique_keywords": data.unique_keywords,
                    "top_keywords": data.top_keywords
                }
            else:
                serializable_data[window_type] = data
                
        await websocket_manager.broadcast({
            "type": "wordcloud_update",
            "data": serializable_data
        })
    
    aggregator.add_update_callback(broadcast_wordcloud_update)
    
    # Kafka Consumer ì‹œì‘
    kafka_consumer.start()
    
    logger.info("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ")

@app.on_event("shutdown")
async def shutdown():
    """ì„œë¹„ìŠ¤ ì¢…ë£Œ"""
    logger.info("ğŸ›‘ ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")
    kafka_consumer.stop()
    logger.info("âœ… ì„œë¹„ìŠ¤ ì¢…ë£Œ ì™„ë£Œ")

# WebSocket ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •
@app.websocket("/ws/keywords")
async def websocket_endpoint(websocket: WebSocket):
    """ì‹¤ì‹œê°„ í‚¤ì›Œë“œ WebSocket"""
    await websocket_manager.connect(websocket)
    
    try:
        # ì—°ê²° ì‹œ í˜„ì¬ ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ì „ì†¡
        for window_type in ["1min", "5min", "15min"]:
            wordcloud_data = await aggregator.get_current_wordcloud(window_type)
            layout = wordcloud_generator.generate_wordcloud_layout(wordcloud_data)
            
            await websocket.send_text(json.dumps({
                "type": "wordcloud_update",
                "data": {
                    window_type: layout
                }
            }, ensure_ascii=False))
        
        # í˜„ì¬ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì „ì†¡
        trending = await analyzer.get_trending_keywords_advanced(10)
        await websocket.send_text(json.dumps({
            "type": "initial_trending",
            "data": [
                {
                    "keyword": t.keyword,
                    "count": t.count_1h,
                    "trend": t.trend_direction,
                    "score": t.compound_score
                }
                for t in trending
            ]
        }, ensure_ascii=False))
        
        # í†µê³„ ì—…ë°ì´íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
        stats_task = asyncio.create_task(stream_stats(websocket))
        
        # ì—°ê²° ìœ ì§€
        while True:
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        stats_task.cancel()
        websocket_manager.disconnect(websocket)

@app.get("/trending-keywords-advanced")
async def get_trending_keywords_advanced(limit: int = 20):
    """ê³ ë„í™”ëœ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¡°íšŒ"""
    trending = await analyzer.get_trending_keywords_advanced(limit)
    
    return {
        "keywords": [
            {
                "keyword": t.keyword,
                "counts": {
                    "1h": t.count_1h,
                    "6h": t.count_6h,
                    "24h": t.count_24h,
                    "7d": t.count_7d
                },
                "velocity": {
                    "1h": t.velocity_1h,
                    "6h": t.velocity_6h
                },
                "anomaly_score": t.anomaly_score,
                "z_score": t.z_score,
                "trend_direction": t.trend_direction,
                "compound_score": t.compound_score
            }
            for t in trending
        ],
        "total_count": len(trending)
    }

@app.get("/keyword-timeline/{keyword}")
async def get_keyword_timeline(keyword: str, hours: int = 24):
    """í‚¤ì›Œë“œ ì‹œê°„ë³„ ë°ì´í„°"""
    timeline = await analyzer.get_timeline_data(keyword, hours)
    return {"keyword": keyword, "timeline": timeline}

@app.get("/alerts")
async def get_recent_alerts(limit: int = 10):
    """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
    alerts = await analyzer.get_recent_alerts(limit)
    return {"alerts": alerts}

@app.get("/stats")
async def get_service_stats():
    """ì„œë¹„ìŠ¤ í†µê³„"""
    extraction_stats = extractor.get_extraction_stats()
    
    return {
        "processed_articles": event_handler.processed_count,
        "active_websockets": len(websocket_manager.active_connections),
        "extraction_stats": extraction_stats,
        "kafka_running": kafka_consumer.running,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/extract-keywords")
async def manual_extract_keywords(request: KeywordRequest):
    """ìˆ˜ë™ í‚¤ì›Œë“œ ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ìš©)"""
    start_time = datetime.now()
    
    keywords = await extractor.extract_keywords(
        request.title, 
        request.content, 
        request.metadata
    )
    
    if keywords:
        await analyzer.add_keywords(keywords, request.category, request.metadata)
    
    processing_time = (datetime.now() - start_time).total_seconds()
    
    return {
        "keywords": keywords,
        "processing_time": processing_time,
        "extraction_method": "hybrid" if extractor.openai_client else "basic"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        await analyzer.redis_client.ping()
        redis_status = True
    except:
        redis_status = False
    
    return {
        "status": "healthy" if (redis_status and kafka_consumer.running) else "degraded",
        "services": {
            "redis": redis_status,
            "kafka_consumer": kafka_consumer.running,
            "websocket_connections": len(websocket_manager.active_connections)
        },
        "kafka_config": {
            "bootstrap_servers": KAFKA_BOOTSTRAP_SERVERS,
            "topic": KAFKA_TOPIC,
            "group_id": KAFKA_GROUP_ID
        },
        "processed_count": event_handler.processed_count
    }
# í†µê³„ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ ì¶”ê°€
async def stream_stats(websocket: WebSocket):
    """ì£¼ê¸°ì  í†µê³„ ì—…ë°ì´íŠ¸"""
    try:
        while True:
            stats = {
                "processed_articles": event_handler.processed_count,
                "active_keywords": len((await aggregator.get_current_wordcloud("5min")).keywords),
                "updates_received": aggregator.stats["updates_sent"],
                "last_update": aggregator.stats["last_update"].isoformat() if aggregator.stats["last_update"] else None
            }
            
            await websocket.send_text(json.dumps({
                "type": "stats_update",
                "data": stats
            }))
            
            await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
            
    except Exception as e:
        logger.error(f"í†µê³„ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")

# ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.get("/wordcloud/{window_type}")
async def get_wordcloud(window_type: str = "5min"):
    """ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ì¡°íšŒ"""
    if window_type not in ["1min", "5min", "15min"]:
        return {"error": "Invalid window type"}
    
    wordcloud_data = await aggregator.get_current_wordcloud(window_type)
    layout = wordcloud_generator.generate_wordcloud_layout(wordcloud_data)
    
    return layout

@app.get("/wordcloud/{window_type}/history")
async def get_wordcloud_history(window_type: str = "5min", hours: int = 1):
    """ì›Œë“œí´ë¼ìš°ë“œ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    history = await aggregator.get_historical_data(window_type, hours)
    return {"window_type": window_type, "history": history}

@app.get("/wordcloud/compare")
async def compare_windows():
    """ë‹¤ì¤‘ ìœˆë„ìš° ë¹„êµ"""
    comparison = {}
    
    for window_type in ["1min", "5min", "15min"]:
        wordcloud_data = await aggregator.get_current_wordcloud(window_type)
        comparison[window_type] = {
            "total_count": wordcloud_data.total_count,
            "unique_keywords": wordcloud_data.unique_keywords,
            "top_5": wordcloud_data.top_keywords[:5]
        }
    
    return comparison

@app.get("/stats")
async def get_service_stats():
    """ì„œë¹„ìŠ¤ í†µê³„ (ì—…ë°ì´íŠ¸)"""
    extraction_stats = extractor.get_extraction_stats()
    aggregator_stats = aggregator.get_stats()
    
    return {
        "processed_articles": event_handler.processed_count,
        "active_websockets": len(websocket_manager.active_connections),
        "extraction_stats": extraction_stats,
        "aggregator_stats": aggregator_stats,
        "kafka_running": kafka_consumer.running,
        "timestamp": datetime.now().isoformat()
    }
# ğŸ”¥ ë””ë²„ê¹…ìš© ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.get("/debug/kafka-status")
async def debug_kafka_status():
    """Kafka ìƒíƒœ ë””ë²„ê¹…"""
    return {
        "consumer_running": kafka_consumer.running,
        "worker_thread_alive": kafka_consumer.worker_thread.is_alive() if kafka_consumer.worker_thread else False,
        "processed_messages": event_handler.processed_count,
        "config": kafka_consumer.kafka_config,
        "topics": kafka_consumer.topics
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=False)  # reload=Falseë¡œ ì„¤ì •