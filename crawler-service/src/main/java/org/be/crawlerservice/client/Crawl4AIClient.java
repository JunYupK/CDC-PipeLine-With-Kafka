package org.be.crawlerservice.client;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.be.crawlerservice.config.CrawlerProperties;
import org.be.crawlerservice.dto.crawl4ai.Crawl4AIRequest;
import org.be.crawlerservice.dto.crawl4ai.Crawl4AIResult;
import org.be.crawlerservice.exception.CrawlException;
import org.be.crawlerservice.service.docker.DockerContainerManager;
import org.springframework.http.*;
import org.springframework.stereotype.Service;
import org.springframework.web.client.RestTemplate;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;

/**
 * Crawl4AI Docker API í´ë¼ì´ì–¸íŠ¸ (ìë™ ë³µêµ¬ ê¸°ëŠ¥ í¬í•¨)
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class Crawl4AIClient {

    private final RestTemplate restTemplate;
    private final CrawlerProperties crawlerProperties;
    private final ObjectMapper objectMapper;
    private final DockerContainerManager dockerManager;

    // ìƒìˆ˜ ì •ì˜
    private static final int DEFAULT_POLL_INTERVAL = 3;
    private static final int DEFAULT_TIMEOUT = 300;
    private static final int MAX_RETRIES = 3;

    /**
     * ìë™ ë³µêµ¬ ê¸°ëŠ¥ì´ ìˆëŠ” Deep Crawling BFS ì‹¤í–‰
     */
    public CompletableFuture<List<Crawl4AIResult>> crawlBFSAsync(String startUrl, int maxDepth, int maxPages, Map<String, Object> schema) {
        return CompletableFuture.supplyAsync(() -> {
            for (int attempt = 1; attempt <= MAX_RETRIES; attempt++) {
                try {
                    log.info("ğŸš€ BFS Deep Crawling ì‹œë„ {}/{}: {}", attempt, MAX_RETRIES, startUrl);

                    // 1. ì‚¬ì „ í—¬ìŠ¤ì²´í¬ ë° ë³µêµ¬
                    if (!ensureContainerHealthy(attempt > 1)) {
                        if (attempt == MAX_RETRIES) {
                            throw new CrawlException("Crawl4AI ì»¨í…Œì´ë„ˆ ë³µêµ¬ ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼)");
                        }
                        continue;
                    }

                    // 2. ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
                    List<Crawl4AIResult> results = executeBFSCrawling(startUrl, maxDepth, maxPages, schema);

                    log.info("âœ… BFS Deep Crawling ì„±ê³µ: {}ê°œ í˜ì´ì§€ ì²˜ë¦¬", results.size());
                    return results;

                } catch (CrawlException e) {
                    log.warn("âš ï¸ BFS Deep Crawling ì‹œë„ {} ì‹¤íŒ¨: {}", attempt, e.getMessage());

                    if (attempt == MAX_RETRIES) {
                        log.error("âŒ BFS Deep Crawling ìµœì¢… ì‹¤íŒ¨: {}", startUrl);
                        throw e;
                    }

                    // ì‹¤íŒ¨ í›„ ë³µêµ¬ ì‹œë„
                    try {
                        log.info("ğŸ”§ ì‹¤íŒ¨ í›„ ë³µêµ¬ ì‹œì‘ (ì‹œë„ {})", attempt);
                        dockerManager.recoverCrawl4AI();
                        Thread.sleep(10000); // 10ì´ˆ ì¶”ê°€ ëŒ€ê¸°
                    } catch (Exception recoveryEx) {
                        log.error("ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {}", recoveryEx.getMessage());
                    }
                }
            }

            throw new CrawlException("BFS Deep Crawling ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼");
        });
    }

    /**
     * ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸ ë° í•„ìš”ì‹œ ë³µêµ¬
     */
    private boolean ensureContainerHealthy(boolean forceRecover) {
        try {
            // 1. Docker ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if (!dockerManager.isDockerAvailable()) {
                log.warn("Docker ëª…ë ¹ì–´ ì‚¬ìš© ë¶ˆê°€ - ë³µêµ¬ ê±´ë„ˆëœ€");
                return true; // Dockerê°€ ì—†ì–´ë„ ì§„í–‰ (ê°œë°œí™˜ê²½ ë“±)
            }

            // 2. ì˜ˆë°©ì  ë©”ëª¨ë¦¬ ì²´í¬
            double memoryUsage = dockerManager.getContainerMemoryUsage("crawl4ai-server");
            if (memoryUsage > 80.0) {
                log.warn("ğŸš¨ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê°ì§€: {}% - ì˜ˆë°©ì  ì¬ì‹œì‘", memoryUsage);
                forceRecover = true;
            }

            // 3. í—¬ìŠ¤ì²´í¬
            if (!forceRecover && dockerManager.isCrawl4AIHealthy()) {
                // ì¶”ê°€ë¡œ API í—¬ìŠ¤ì²´í¬
                if (isApiHealthy()) {
                    log.debug("âœ… Crawl4AI ìƒíƒœ ì–‘í˜¸");
                    return true;
                }
            }

            // 4. ë³µêµ¬ ì‹¤í–‰
            log.info("ğŸ”§ Crawl4AI ì»¨í…Œì´ë„ˆ ë³µêµ¬ ì‹œì‘");
            boolean recovered = dockerManager.recoverCrawl4AI();

            if (!recovered) {
                log.error("âŒ ì»¨í…Œì´ë„ˆ ë³µêµ¬ ì‹¤íŒ¨");
                return false;
            }

            // 5. API ì¤€ë¹„ ëŒ€ê¸°
            return waitForApiReady(30);

        } catch (Exception e) {
            log.error("ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜", e);
            return false;
        }
    }

    /**
     * API í—¬ìŠ¤ì²´í¬
     */
    private boolean isApiHealthy() {
        try {
            String healthUrl = crawlerProperties.getCrawl4aiUrl() + "/health";
            ResponseEntity<String> response = restTemplate.getForEntity(healthUrl, String.class);
            return response.getStatusCode() == HttpStatus.OK;
        } catch (Exception e) {
            log.debug("API í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {}", e.getMessage());
            return false;
        }
    }

    /**
     * API ì¤€ë¹„ ìƒíƒœê¹Œì§€ ëŒ€ê¸°
     */
    private boolean waitForApiReady(int timeoutSeconds) {
        log.info("â³ Crawl4AI API ì¤€ë¹„ ëŒ€ê¸° ({}ì´ˆ)", timeoutSeconds);

        long startTime = System.currentTimeMillis();
        long timeoutMillis = timeoutSeconds * 1000L;

        while (System.currentTimeMillis() - startTime < timeoutMillis) {
            if (isApiHealthy()) {
                long elapsed = (System.currentTimeMillis() - startTime) / 1000;
                log.info("âœ… Crawl4AI API ì¤€ë¹„ ì™„ë£Œ ({}ì´ˆ)", elapsed);
                return true;
            }

            try {
                Thread.sleep(3000); // 3ì´ˆë§ˆë‹¤ ì¬í™•ì¸
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                return false;
            }
        }

        log.error("âŒ API ì¤€ë¹„ íƒ€ì„ì•„ì›ƒ ({}ì´ˆ)", timeoutSeconds);
        return false;
    }

    /**
     * ì‹¤ì œ BFS í¬ë¡¤ë§ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§)
     */
    private List<Crawl4AIResult> executeBFSCrawling(String startUrl, int maxDepth, int maxPages, Map<String, Object> schema) {
        try {
            log.debug("BFS Deep Crawling ì‹¤í–‰: url={}, depth={}, pages={}", startUrl, maxDepth, maxPages);

            //Crawl4AIRequest request = Crawl4AIRequest.forBFSDeepCrawl(startUrl, maxDepth, maxPages, schema);
            Crawl4AIRequest request = Crawl4AIRequest.forBestFirstNaverNews(startUrl, maxDepth, maxPages, schema);
            String crawlUrl = crawlerProperties.getCrawl4aiUrl() + "/crawl";
            HttpHeaders headers = createHeaders();
            String requestJson = objectMapper.writeValueAsString(request);
            HttpEntity<String> requestEntity = new HttpEntity<>(requestJson, headers);

            ResponseEntity<String> response = restTemplate.postForEntity(crawlUrl, requestEntity, String.class);

            if (response.getStatusCode() != HttpStatus.OK || response.getBody() == null) {
                throw new CrawlException("API í˜¸ì¶œ ì‹¤íŒ¨: " + response.getStatusCode());
            }

            JsonNode responseJson = objectMapper.readTree(response.getBody());
            JsonNode results = responseJson.get("results");
            List<Crawl4AIResult> crawlResults = new ArrayList<>();

            for (JsonNode resultNode : results) {
                Crawl4AIResult.CrawlResult crawlResult = parseCrawlResult(resultNode);
                Crawl4AIResult result = Crawl4AIResult.builder()
                        .result(crawlResult)
                        .completedTime(LocalDateTime.now())
                        .build();
                crawlResults.add(result);
            }

            return crawlResults;

        } catch (Exception e) {
            throw new CrawlException("BFS Deep Crawling ì‹¤í–‰ ì‹¤íŒ¨: " + e.getMessage(), e);
        }
    }

    // === ê¸°ì¡´ ë©”ì„œë“œë“¤ (í˜¸í™˜ì„± ìœ ì§€) ===

    /**
     * ê¸°ë³¸ ë™ê¸° í¬ë¡¤ë§ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€)
     */
    public Crawl4AIResult crawl(Crawl4AIRequest request, boolean flag) {
        if(flag) return crawl(request, DEFAULT_POLL_INTERVAL, DEFAULT_TIMEOUT);
        else return crawl(request);
    }

    public Crawl4AIResult crawl(Crawl4AIRequest request, int pollInterval, int timeoutSeconds) {
        try {
            log.info("í¬ë¡¤ë§ ì‹œì‘: URLs={}", request.getUrls());
            String requestJson = objectMapper.writeValueAsString(request);
            String crawlUrl = crawlerProperties.getCrawl4aiUrl() + "/crawl";
            HttpHeaders headers = createHeaders();
            HttpEntity<String> requestEntity = new HttpEntity<>(requestJson, headers);

            ResponseEntity<String> response = restTemplate.postForEntity(crawlUrl, requestEntity, String.class);

            if (response.getStatusCode() == HttpStatus.OK && response.getBody() != null) {
                return parseStatusResponse(response.getBody());
            }
        } catch (Exception e) {
            log.error("í¬ë¡¤ë§ ì‘ì—… ì‹¤íŒ¨: URLs={}", request.getUrls(), e);
            throw new CrawlException("Crawl operation failed: " + e.getMessage(), e);
        }
        return null;
    }

    public Crawl4AIResult crawl(Crawl4AIRequest request) {
        return crawl(request, false);
    }

    /**
     * í—¬ìŠ¤ ì²´í¬ (ì™¸ë¶€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
     */
    public boolean isHealthy() {
        return isApiHealthy();
    }

    // === Private í—¬í¼ ë©”ì„œë“œë“¤ ===

    private String getTextValue(JsonNode node, String fieldName) {
        if (node.has(fieldName) && !node.get(fieldName).isNull()) {
            String value = node.get(fieldName).asText();
            return value.isEmpty() ? null : value;
        }
        return null;
    }

    private HttpHeaders createHeaders() {
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);
        if (crawlerProperties.getApiToken() != null && !crawlerProperties.getApiToken().isEmpty()) {
            headers.setBearerAuth(crawlerProperties.getApiToken());
        }
        return headers;
    }

    private Crawl4AIResult parseStatusResponse(String responseBody) throws JsonProcessingException {
        JsonNode responseJson = objectMapper.readTree(responseBody);
        Crawl4AIResult.Crawl4AIResultBuilder resultBuilder = Crawl4AIResult.builder();
        JsonNode resultNode = responseJson.get("results");
        JsonNode firstResult = resultNode.get(0);
        Crawl4AIResult.CrawlResult crawlResult = parseCrawlResult(firstResult);
        resultBuilder.result(crawlResult).completedTime(LocalDateTime.now());
        return resultBuilder.build();
    }

    /**
     * í¬ë¡¤ë§ ê²°ê³¼ íŒŒì‹±
     */
    private Crawl4AIResult.CrawlResult parseCrawlResult(JsonNode resultNode) {
        Crawl4AIResult.CrawlResult.CrawlResultBuilder builder = Crawl4AIResult.CrawlResult.builder();

        builder.url(getTextValue(resultNode, "url"));
        builder.html(getTextValue(resultNode, "html"));
        builder.cleanedHtml(getTextValue(resultNode, "cleaned_html"));

        // markdown ì²˜ë¦¬
        if (resultNode.has("markdown") && !resultNode.get("markdown").isNull()) {
            JsonNode markdownNode = resultNode.get("markdown");
            if (markdownNode.isObject()) {
                String rawMarkdown = getTextValue(markdownNode, "raw_markdown");
                builder.markdown(rawMarkdown);
            } else {
                builder.markdown(markdownNode.asText());
            }
        }

        // extracted_content ì²˜ë¦¬
        if (resultNode.has("extracted_content")) {
            JsonNode extractedNode = resultNode.get("extracted_content");
            if (!extractedNode.isNull()) {
                String extracted = extractedNode.asText();
                builder.extractedContent(extracted.isEmpty() ? null : extracted);
            }
        }

        // ì„±ê³µ ì—¬ë¶€
        if (resultNode.has("success")) {
            builder.success(resultNode.get("success").asBoolean());
        }

        return builder.build();
    }
}