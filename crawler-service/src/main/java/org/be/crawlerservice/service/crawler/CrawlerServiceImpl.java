package org.be.crawlerservice.service.crawler;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.be.crawlerservice.client.Crawl4AIClient;
import org.be.crawlerservice.client.schema.NaverNewsSchemas;
import org.be.crawlerservice.dto.crawl4ai.Crawl4AIRequest;
import org.be.crawlerservice.dto.crawl4ai.Crawl4AIResult;
import org.be.crawlerservice.dto.request.CrawlRequestDto;
import org.be.crawlerservice.dto.response.CrawlStatusDto;
import org.be.crawlerservice.dto.response.StatsResponseDto;
import org.be.crawlerservice.entity.Article;
import org.be.crawlerservice.enums.CrawlerStatus;
import org.be.crawlerservice.metrics.CrawlerMetrics;
import org.be.crawlerservice.service.article.ArticleService;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import org.springframework.util.StringUtils;

import java.net.MalformedURLException;
import java.net.URL;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicReference;

@Slf4j
@Service
@RequiredArgsConstructor
public class CrawlerServiceImpl implements CrawlerService {

    private final ArticleService articleService;
    private final CrawlerMetrics crawlerMetrics;
    private final Crawl4AIClient crawl4AIClient;
    private final ObjectMapper objectMapper;

    // í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬
    private final AtomicBoolean isCrawling = new AtomicBoolean(false);
    private final AtomicReference<String> currentCategory = new AtomicReference<>();
    private final AtomicReference<LocalDateTime> crawlStartTime = new AtomicReference<>();
    private final Map<String, Integer> errorCounts = new ConcurrentHashMap<>();
    private final Map<String, LocalDateTime> lastExecutionTimes = new ConcurrentHashMap<>();
    private CompletableFuture<Void> currentCrawlTask;

    @Override
    public CrawlStatusDto startCrawling(CrawlRequestDto request) {
        if (isCrawling.get()) {
            throw new RuntimeException("í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤");
        }

        log.info("í¬ë¡¤ë§ ì‹œì‘ ìš”ì²­: category={}", request.getCategory());

        // í¬ë¡¤ë§ ìƒíƒœ ì„¤ì •
        isCrawling.set(true);
        currentCategory.set(request.getCategory());
        crawlStartTime.set(LocalDateTime.now());

        // ë¹„ë™ê¸°ë¡œ í¬ë¡¤ë§ ì‘ì—… ì‹œì‘
        currentCrawlTask = crawlAsync(request);

        return CrawlStatusDto.builder()
                .status(CrawlerStatus.RUNNING)
                .currentCategory(request.getCategory())
                .startTime(crawlStartTime.get())
                .message("í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤")
                .build();
    }

    @Override
    public CrawlStatusDto stopCrawling() {
        if (!isCrawling.get()) {
            throw new RuntimeException("ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ë§ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤");
        }

        log.info("í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­");

        // í˜„ì¬ ì‘ì—… ì·¨ì†Œ
        if (currentCrawlTask != null && !currentCrawlTask.isDone()) {
            currentCrawlTask.cancel(true);
        }

        // ìƒíƒœ ì´ˆê¸°í™”
        resetCrawlingState();

        return CrawlStatusDto.builder()
                .status(CrawlerStatus.IDLE)
                .message("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
                .build();
    }

    @Override
    public CrawlStatusDto getCurrentStatus() {
        if (isCrawling.get()) {
            return CrawlStatusDto.builder()
                    .status(CrawlerStatus.RUNNING)
                    .currentCategory(currentCategory.get())
                    .startTime(crawlStartTime.get())
                    .errorCounts(new HashMap<>(errorCounts))
                    .lastExecutionTimes(new HashMap<>(lastExecutionTimes))
                    .message("í¬ë¡¤ë§ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
                    .build();
        } else {
            return CrawlStatusDto.builder()
                    .status(CrawlerStatus.IDLE)
                    .errorCounts(new HashMap<>(errorCounts))
                    .lastExecutionTimes(new HashMap<>(lastExecutionTimes))
                    .message("í¬ë¡¤ë§ ëŒ€ê¸° ì¤‘")
                    .build();
        }
    }

    @Override
    public StatsResponseDto getCrawlingStats() {
        return articleService.getArticleStats();
    }

    @Override
    public Map<String, String> getLastExecutionTimes() {
        Map<String, String> result = new HashMap<>();
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

        lastExecutionTimes.forEach((category, time) -> {
            result.put(category, time.format(formatter));
        });

        return result;
    }

    @Override
    public Map<String, Double> getSuccessRates() {
        Map<String, Double> successRates = new HashMap<>();

        NaverNewsSchemas.getCategoryUrls().keySet().forEach(category -> {
            double successRate = calculateSuccessRate(category);
            successRates.put(category, successRate);
        });

        return successRates;
    }

    /**
     * ë¹„ë™ê¸° í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰
     */
    @Async("crawlerExecutor")
    protected CompletableFuture<Void> crawlAsync(CrawlRequestDto request) {
        return CompletableFuture.runAsync(() -> {
            try {
                crawlJob(request.getCategory());
            } catch (Exception e) {
                log.error("í¬ë¡¤ë§ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ", e);
                handleCrawlingError(currentCategory.get(), e);
            } finally {
                resetCrawlingState();
            }
        });
    }

    /**
     * ì‹¤ì œ í¬ë¡¤ë§ ì‘ì—… ìˆ˜í–‰ (Pythonì˜ crawling_jobê³¼ ë™ì¼í•œ ë¡œì§)
     * ğŸ”¥ ì‹¤ì œ Crawl4AIClient ì‚¬ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸
     */
    private void crawlJob(String targetCategory) {
        log.info("í¬ë¡¤ë§ ì‘ì—… ì‹œì‘: targetCategory={}", targetCategory);

        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));

        // ëŒ€ìƒ URL í•„í„°ë§
        Map<String, String> categoryUrls = NaverNewsSchemas.getCategoryUrls();
        Map<String, String> urlsToProcess = new HashMap<>();

        if (targetCategory == null) {
            // ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
            urlsToProcess.putAll(categoryUrls);
        } else if (categoryUrls.containsKey(targetCategory)) {
            // íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§
            urlsToProcess.put(targetCategory, categoryUrls.get(targetCategory));
        } else {
            log.warn("Unknown category: {}", targetCategory);
            return;
        }

        for (Map.Entry<String, String> entry : urlsToProcess.entrySet()) {
            if (Thread.currentThread().isInterrupted()) {
                log.info("í¬ë¡¤ë§ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤");
                break;
            }

            String category = entry.getKey();
            String url = entry.getValue();

            try {
                log.info("ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘: {} - {}", category, timestamp);

                // ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸: í¬ë¡¤ë§ ì‹œì‘
                crawlerMetrics.updateCrawlStatus(category, true);
                currentCategory.set(category);

                long startTime = System.currentTimeMillis();

                // 1. ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ (ì‹¤ì œ êµ¬í˜„)
                List<Article> articles = crawlNewsMetadata(url, category, timestamp);

                if (articles != null && !articles.isEmpty()) {
                    // 2. ë‚´ìš© í¬ë¡¤ë§ (ì‹¤ì œ êµ¬í˜„)
                    List<Article> articlesWithContent = crawlArticleContents(articles, category, timestamp);

                    if (articlesWithContent != null && !articlesWithContent.isEmpty()) {
                        // 3. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                        saveArticlesToDatabase(articlesWithContent, category);

                        // 4. ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                        long crawlTime = System.currentTimeMillis() - startTime;
                        updateSuccessMetrics(category, articlesWithContent.size(), crawlTime);

                        log.info("{} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ: {}ê°œ ê¸°ì‚¬ ì²˜ë¦¬", category, articlesWithContent.size());
                    } else {
                        log.warn("{} ì¹´í…Œê³ ë¦¬ ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨", category);
                        updateFailureMetrics(category);
                    }
                } else {
                    log.warn("{} ì¹´í…Œê³ ë¦¬ ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ ì‹¤íŒ¨", category);
                    updateFailureMetrics(category);
                }

            } catch (Exception e) {
                log.error("{} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", category, e);
                handleCrawlingError(category, e);
            } finally {
                // í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™”
                crawlerMetrics.updateCrawlStatus(category, false);
                lastExecutionTimes.put(category, LocalDateTime.now());
            }

            // ì¹´í…Œê³ ë¦¬ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            try {
                Thread.sleep(2000); // 2ì´ˆ ëŒ€ê¸°
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            }
        }

        log.info("ì „ì²´ í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ");
    }

    /**
     * ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ (ì‹¤ì œ Crawl4AIClient ì‚¬ìš©)
     * ğŸ”¥ ì‹¤ì œ êµ¬í˜„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
     */
    private List<Article> crawlNewsMetadata(String url, String category, String timestamp) {
        log.debug("ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘: {}", url);

        try {
            // Crawl4AI ìš”ì²­ ìƒì„±
            Map<String, Object> schema = NaverNewsSchemas.getSchemaForCategory(category, true);
            Crawl4AIRequest request = Crawl4AIRequest.forUrlList(url, schema);

            // ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
            Crawl4AIResult result = crawl4AIClient.crawl(request);

            if (!result.isCrawlSuccessful()) {
                log.error("ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {} - {}", url, result.getError());
                return null;
            }

            if (!result.hasExtractedContent()) {
                log.warn("ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ ì—†ìŒ: {}", url);
                return null;
            }

            // JSON íŒŒì‹±
            List<Map<String, Object>> extractedItems = objectMapper.readValue(
                    result.getResult().getExtractedContent(),
                    new TypeReference<List<Map<String, Object>>>() {}
            );

            // Article ì—”í‹°í‹°ë¡œ ë³€í™˜
            List<Article> articles = new ArrayList<>();
            for (Map<String, Object> item : extractedItems) {
                String title = (String) item.get("title");
                String link = (String) item.get("link");

                // ìœ íš¨ì„± ê²€ì‚¬
                if (!StringUtils.hasText(title) || !StringUtils.hasText(link)) {
                    continue;
                }

                // ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                String absoluteLink = convertToAbsoluteUrl(link, url);

                Article article = Article.builder()
                        .title(title.trim())
                        .link(absoluteLink)
                        .category(category)
                        .storedDate(timestamp.substring(0, 8)) // YYYYMMDD í˜•ì‹
                        .source("ë„¤ì´ë²„ë‰´ìŠ¤")
                        .content("") // ë‚˜ì¤‘ì— ì±„ì›€
                        .build();

                articles.add(article);
            }

            log.info("ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ: {}ê°œ ê¸°ì‚¬", articles.size());
            return articles;

        } catch (JsonProcessingException e) {
            log.error("JSON íŒŒì‹± ì‹¤íŒ¨: {}", url, e);
            return null;
        } catch (Exception e) {
            log.error("ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {}", url, e);
            return null;
        }
    }

    /**
     * ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ (ì‹¤ì œ Crawl4AIClient ì‚¬ìš©)
     * ğŸ”¥ ì‹¤ì œ êµ¬í˜„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
     */
    private List<Article> crawlArticleContents(List<Article> articles, String category, String timestamp) {
        log.debug("ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì‹œì‘: {}ê°œ ê¸°ì‚¬", articles.size());

        Map<String, Object> contentSchema = NaverNewsSchemas.getSchemaForCategory(category, false);
        List<Article> successfulArticles = new ArrayList<>();

        for (Article article : articles) {
            if (Thread.currentThread().isInterrupted()) {
                break;
            }

            try {
                // Crawl4AI ìš”ì²­ ìƒì„±
                Crawl4AIRequest request = Crawl4AIRequest.forArticleContent(article.getLink(), contentSchema);

                // ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
                Crawl4AIResult result = crawl4AIClient.crawl(request);

                if (result.isCrawlSuccessful() && result.hasExtractedContent()) {
                    // JSON íŒŒì‹±
                    List<Map<String, Object>> extractedContent = objectMapper.readValue(
                            result.getResult().getExtractedContent(),
                            new TypeReference<List<Map<String, Object>>>() {}
                    );

                    if (!extractedContent.isEmpty()) {
                        Map<String, Object> contentData = extractedContent.get(0);
                        String content = (String) contentData.get("content");

                        if (StringUtils.hasText(content)) {
                            // ê¸°ì‚¬ ë‚´ìš© ì—…ë°ì´íŠ¸
                            article.setContent(content.trim());
                            article.setArticleTextLength(content.length());
                            successfulArticles.add(article);

                            log.debug("ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì„±ê³µ: {}", article.getTitle());
                        }
                    }
                } else {
                    log.warn("ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨: {} - {}", article.getLink(), result.getError());
                }

                // ë”œë ˆì´ (ë´‡ ê°ì§€ ë°©ì§€)
                Thread.sleep(1500); // 1.5ì´ˆ ëŒ€ê¸°

            } catch (JsonProcessingException e) {
                log.error("JSON íŒŒì‹± ì‹¤íŒ¨: {}", article.getLink(), e);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            } catch (Exception e) {
                log.error("ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨: {}", article.getLink(), e);
            }
        }

        log.info("ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì™„ë£Œ: {}/{}ê°œ ì„±ê³µ", successfulArticles.size(), articles.size());
        return successfulArticles;
    }

    /**
     * ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
     */
    private String convertToAbsoluteUrl(String link, String baseUrl) {
        if (link.startsWith("http")) {
            return link;
        }

        try {
            URL base = new URL(baseUrl);
            if (link.startsWith("/")) {
                return base.getProtocol() + "://" + base.getHost() + link;
            } else {
                return baseUrl + "/" + link;
            }
        } catch (MalformedURLException e) {
            log.warn("URL ë³€í™˜ ì‹¤íŒ¨: {} + {}", baseUrl, link);
            return link;
        }
    }

    /**
     * ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ ì €ì¥
     */
    private void saveArticlesToDatabase(List<Article> articles, String category) {
        try {
            long startTime = System.currentTimeMillis();

            List<Article> savedArticles = articleService.saveArticles(articles);

            long saveTime = System.currentTimeMillis() - startTime;
            crawlerMetrics.recordDbOperationTime(saveTime);

            log.info("{} ì¹´í…Œê³ ë¦¬ {}ê°œ ê¸°ì‚¬ DB ì €ì¥ ì™„ë£Œ ({}ms)",
                    category, savedArticles.size(), saveTime);

        } catch (Exception e) {
            log.error("{} ì¹´í…Œê³ ë¦¬ DB ì €ì¥ ì‹¤íŒ¨", category, e);
            throw new RuntimeException("DB ì €ì¥ ì‹¤íŒ¨", e);
        }
    }

    /**
     * ì„±ê³µ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
     */
    private void updateSuccessMetrics(String category, int articleCount, long crawlTime) {
        crawlerMetrics.incrementCrawlSuccess(category);
        crawlerMetrics.incrementArticlesProcessed(category, articleCount);
        crawlerMetrics.recordCrawlTime(category, crawlTime);

        // ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹
        errorCounts.put(category, 0);
    }

    /**
     * ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
     */
    private void updateFailureMetrics(String category) {
        crawlerMetrics.incrementCrawlFailure(category);

        // ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¦ê°€
        errorCounts.merge(category, 1, Integer::sum);
    }

    /**
     * í¬ë¡¤ë§ ì˜¤ë¥˜ ì²˜ë¦¬
     */
    private void handleCrawlingError(String category, Exception e) {
        updateFailureMetrics(category);
        log.error("í¬ë¡¤ë§ ì˜¤ë¥˜ ì²˜ë¦¬: category={}", category, e);
    }

    /**
     * í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™”
     */
    private void resetCrawlingState() {
        isCrawling.set(false);
        currentCategory.set(null);
        crawlStartTime.set(null);
        currentCrawlTask = null;
    }

    /**
     * ì„±ê³µë¥  ê³„ì‚°
     */
    private double calculateSuccessRate(String category) {
        int errorCount = errorCounts.getOrDefault(category, 0);
        return errorCount == 0 ? 95.0 : Math.max(50.0, 95.0 - (errorCount * 10));
    }
}